{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "F = torch.eye(5).to(device)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a single linear attention unit for linear-regression data\n",
    "# the dimensions of the input are\n",
    "# B: batch-size of prompts\n",
    "# N: context length (excluding query)\n",
    "# d: covariate dimension\n",
    "# V is a (d+1) x (d+1) matrix\n",
    "# B,C are d x d matrices\n",
    "# Z is a B x (N+1) + (d+1) matrix\n",
    "# Output is also B x (N+1) + (d+1)\n",
    "\n",
    "def gb_activation(U,W):\n",
    "    res = torch.empty((U.shape[1],U.shape[1]))\n",
    "\n",
    "    for i in range(U.shape[1]):\n",
    "        for j in range(W.shape[1]):\n",
    "            vec = torch.mul(U[0: , i:(i+1)], W[0: , j:(j+1)]).detach().to(device).numpy()\n",
    "            res[i,j] = np.prod(np.heaviside(vec, 0))\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "\n",
    "def attention(V, B, C, Z, activation = gb_activation):\n",
    "    # B= Z.shape[0]\n",
    "    N = Z.shape[0]-1\n",
    "    d = Z.shape[1]-1\n",
    "\n",
    "    X = Z[0:d,0:(N+1)]\n",
    "\n",
    "    Attn = activation(torch.einsum('ik, kj -> ij', (B,X)), torch.einsum('ik, kj -> ij', (C,X)))\n",
    "    key = torch.einsum('ik, kj -> ij', (V,Z))\n",
    "\n",
    "    M = torch.eye(N+1).to(device)\n",
    "    M[N,N] = 0\n",
    "\n",
    "    Output = torch.einsum('ik,kl,lj -> ij', (key,M,Attn))\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Linear Transformer module\n",
    "# n_layer denotes the number of layers\n",
    "# n_head denotes the number of heads. In most of our experiments, n_head = 1\n",
    "# d denotes the dimension of covariates\n",
    "# var denotes the variance of initialization. It needs to be sufficiently small, but exact value is not important\n",
    "# allparam: contains all the parameters, has dimension n_layer x n_head x 2 x d x d\n",
    "# For example\n",
    "# - P matrix at layer i, head j is allparam[i,j,0,:,:]\n",
    "# - Q matrix at layer i, head j is allparam[i,j,1,:,:]\n",
    "class Transformer_F(nn.Module):\n",
    "    def __init__(self, n_layer, n_head, d, var):\n",
    "        super(Transformer_F, self).__init__()\n",
    "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d, d)))\n",
    "        with torch.no_grad():\n",
    "            self.allparam.normal_(0,var)\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, Z):\n",
    "        for i in range(self.n_layer):\n",
    "            Zi = Z\n",
    "            residues = 0\n",
    "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
    "            for j in range(self.n_head):\n",
    "                Vij = self.allparam[i,j,0,:,:]\n",
    "                Bij = self.allparam[i,j,1,:,:]\n",
    "                Cij = self.allparam[i,j,2,:,:]\n",
    "                residues = residues + attention(Vij,Bij,Cij,Zi)\n",
    "            Z = Zi + residues\n",
    "        return Z\n",
    "    \n",
    "    #enforces top-left-dxd-block sparsity on p\n",
    "    def zero_p(self):\n",
    "        for i in range(self.n_layer):\n",
    "            for j in range(self.n_head):\n",
    "                with torch.no_grad():\n",
    "                    self.allparam[i,j,0,:,:].zero_()\n",
    "\n",
    "# evaluate the loss of model, given data (Z,y)\n",
    "def in_context_loss(model, Z, y):\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    output = model(Z)\n",
    "    diff = output[:,N,d]+y\n",
    "    loss = ((diff)**2).mean() \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data for linear regression\n",
    "# mode: distribution of samples to generate. Currently supports 'normal', 'gamma', 'sphere'\n",
    "# N: number of context examples\n",
    "# d: dimension of covariates\n",
    "# For gamma distribution:\n",
    "# - shape_k: shape parameter of gamma distribution (unused otherwise)\n",
    "# - scale parameter: hard coded so that when shape_k = 5/2 and d=5, the generated data is standard normal\n",
    "def generate_data(mode='normal',N=20,d=1,B=1000,shape_k=0.1, U=None, D=None):\n",
    "    W= torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
    "    X_test = torch.FloatTensor(B,1,d).normal_(0, 1).to(device)\n",
    "    \n",
    "    if U is not None:\n",
    "        U = U.to(device)\n",
    "        D = D.to(device)\n",
    "        W= torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "        W = torch.mm(W,torch.inverse(D))\n",
    "        W = torch.mm(W,U.t())\n",
    "    \n",
    "    if mode =='sphere':\n",
    "        X.div_(X.norm(p=2,dim=2)[:,:,None])\n",
    "        X_test.div_(X_test.norm(p=2,dim=2)[:,:,None])\n",
    "    elif mode == 'gamma':\n",
    "        # random gamma scaling for X\n",
    "        gamma_scales = np.random.gamma(shape=shape_k, scale=(10/shape_k)**(0.5), size=[B,N])\n",
    "        gamma_scales = torch.Tensor(gamma_scales).to(device)\n",
    "        gamma_scales = gamma_scales.sqrt()\n",
    "        # random gamma scaling for X_test\n",
    "        gamma_test_scales = np.random.gamma(shape=shape_k, scale=(10/shape_k)**(0.5), size=[B,1])\n",
    "        gamma_test_scales = torch.Tensor(gamma_test_scales).to(device)\n",
    "        gamma_test_scales = gamma_test_scales.sqrt()\n",
    "        # normalize to unit norm\n",
    "        X.div_(X.norm(p=2,dim=2)[:,:,None])\n",
    "        X_test.div_(X_test.norm(p=2,dim=2)[:,:,None])\n",
    "        # scale by gamma\n",
    "        X.mul_(gamma_scales[:,:,None])\n",
    "        X_test.mul_(gamma_test_scales[:,:,None])\n",
    "    elif mode =='normal':\n",
    "        assert True\n",
    "    elif mode == 'relu':\n",
    "        return generate_data_relu(N=N, d=d, B=B, hidden_dim=d)\n",
    "    elif mode == 'mlp':\n",
    "        generate_data_mlp(N=N, d=d, B=B, hidden_dim=d)\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    if U is not None:\n",
    "        X = torch.einsum('ij, jk, BNk -> BNi', (U,D,X))\n",
    "        X_test = torch.einsum('ij, jk, BNk -> BNi', (U,D,X_test))\n",
    "        \n",
    "    y = torch.einsum('bi,bni->bn', (W, X)).unsqueeze(2)\n",
    "    y_zero = torch.zeros(B,1,1).to(device)\n",
    "    y_test = torch.einsum('bi,bni->bn', (W, X_test)).squeeze(1)\n",
    "    X_comb= torch.cat([X,X_test],dim=1)\n",
    "    y_comb= torch.cat([y,y_zero],dim=1)\n",
    "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
    "    return Z.to(device),y_test.to(device)\n",
    "\n",
    "def generate_data_inplace(Z, U=None, D=None):\n",
    "    \n",
    "    B = Z.shape[0]\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    X = Z[:,:,0:-1]\n",
    "    X.normal_(0, 1).to(device)\n",
    "    W= torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "    if U is not None:\n",
    "        U = U.to(device)\n",
    "        D = D.to(device)\n",
    "        W = torch.mm(W,torch.inverse(D))\n",
    "        W = torch.mm(W,U.t())\n",
    "        Z[:,:,0:-1] = torch.einsum('ij, jk, BNk -> BNi', (U,D,X))\n",
    "        \n",
    "    Z[:,:,-1] = torch.einsum('bi,bni->bn', (W, Z[:,:,0:-1])) #y update\n",
    "    y_test = Z[:,-1,-1].detach().clone()\n",
    "    Z[:,-1,-1].zero_()\n",
    "    return Z.to(device),y_test.to(device)\n",
    "\n",
    "def generate_data_sine(N=10, B=1000):\n",
    "    # Sample amplitude a and phase p for each task\n",
    "    a = torch.FloatTensor(B).uniform_(0.1, 5).to(device)\n",
    "    p = torch.FloatTensor(B).uniform_(0, math.pi).to(device)\n",
    " \n",
    "    X = torch.FloatTensor(B, N).uniform_(-5, 5).to(device)\n",
    " \n",
    "    Y = a.unsqueeze(1) * torch.sin(p.unsqueeze(1) + X)\n",
    " \n",
    "    X = X.unsqueeze(-1)\n",
    "    Y = Y.unsqueeze(-1)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def generate_data_relu(mode='normal', N=20, d=1, B=1000, shape_k=0.1, U=None, D=None, hidden_dim=100):\n",
    "    # Generate random input data\n",
    "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
    "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
    "\n",
    "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
    "\n",
    "    # Define a 1-hidden layer ReLU network\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(d, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, 1)\n",
    "    ).to(device)\n",
    "    model[0].weight.data.normal_(0, 0.1)\n",
    "    model[2].weight.data.normal_(0, 0.1)\n",
    "\n",
    "    # Generate y values using the ReLU network\n",
    "    y = model(X.view(-1, d)).view(B, N, 1)\n",
    "    y_test = model(X_test.view(-1, d)).view(B, 1).squeeze(1)\n",
    " \n",
    "    y_zero = torch.zeros(B, 1, 1).to(device)\n",
    "    X_comb = torch.cat([X, X_test], dim=1)\n",
    "    y_comb = torch.cat([y, y_zero], dim=1)\n",
    "    Z = torch.cat([X_comb, y_comb], dim=2)\n",
    "\n",
    "    return Z, y_test\n",
    "\n",
    "def generate_data_mlp(N=20, d=1, B=1000, hidden_dim=100):\n",
    "    # Generate random input data\n",
    "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
    "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
    "\n",
    "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
    "\n",
    "    # Define a 1-hidden layer ReLU network\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(d, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, d)\n",
    "    ).to(device)\n",
    "    model[0].weight.data.normal_(0, 1)\n",
    "    model[2].weight.data.normal_(0, 1)\n",
    "\n",
    "    X_MLP = model(X.view(-1, d)).view(B, N, d)\n",
    "    X_test_MLP = model(X_test.view(-1, d)).view(B, 1, d)\n",
    "\n",
    "    W = torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "    y = torch.einsum('bi,bni->bn', (W, X_MLP)).unsqueeze(2)\n",
    "    y_zero = torch.zeros(B,1,1).to(device)\n",
    "    y_test = torch.einsum('bi,bni->bn', (W, X_test_MLP)).squeeze(1)\n",
    "    X_comb= torch.cat([X_MLP,X_test_MLP],dim=1)\n",
    "    y_comb= torch.cat([y,y_zero],dim=1)\n",
    "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
    "\n",
    "    return Z, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
