{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where xgboost works better\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import xgboost as xg \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal([0, 0], [[1, 0.9], [0.9, 1]], 10000)\n",
    "y= np.sin(X[:,0])/np.sqrt(np.abs(X[:,1])) + np.random.normal(0, 0.1, size=len(X))+0*np.random.standard_cauchy(size=10000)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                      test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.20253065288342048\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# Instantiation \n",
    "xgb_r = xg.XGBRegressor(objective ='reg:squarederror', max_depth=10, subsample=0.7,learning_rate = 0.5, alpha = 0,\n",
    "                  n_estimators = 10, seed = 123) \n",
    "  \n",
    "# Fitting the model \n",
    "xgb_r.fit(train_X, train_y) \n",
    "  \n",
    "# Predict the model \n",
    "pred = xgb_r.predict(test_X) \n",
    "  \n",
    "# MSE Computation \n",
    "mse = MSE(test_y, pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868370196789628"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-mse/np.var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aytijhyasaha/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.4356064743788615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# Fit the kernel regression model\n",
    "alpha = 0  # Regularization parameter\n",
    "gamma = 0.01  # Kernel coefficient\n",
    "kernel_reg = KernelRidge(alpha=alpha, kernel='sigmoid', gamma=gamma)\n",
    "kernel_reg.fit(train_X, train_y)\n",
    "\n",
    "# Predict on the training and testing data\n",
    "#y_train_pred = kernel_reg.predict(X_train)\n",
    "y_test_pred = kernel_reg.predict(test_X)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "#mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse = MSE(test_y, y_test_pred)\n",
    "\n",
    "#print(\"Train MSE:\", mse_train)\n",
    "print(\"Test MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7399993092469737"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-mse/np.var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Kernel: 17163.161995852795\n",
      "Test MSE XGB: 20827.24407096688\n",
      "Feature Importance (XGBoost): [0.00045764 0.00375296 0.00668435 0.01252005 0.00090444 0.00466162\n",
      " 0.00511679 0.00986094 0.00727124 0.06434807 0.         0.\n",
      " 0.00073222 0.01021054 0.00538007 0.         0.         0.00356121\n",
      " 0.         0.01080412 0.         0.         0.         0.\n",
      " 0.         0.08665816 0.03306115 0.00779734 0.         0.\n",
      " 0.         0.01016159 0.00190547 0.         0.         0.00297869\n",
      " 0.01845535 0.04127377 0.         0.         0.         0.0472335\n",
      " 0.04274406 0.00775029 0.         0.00442255 0.01065409 0.\n",
      " 0.03020735 0.         0.03260812 0.         0.00403336 0.01253633\n",
      " 0.00701374 0.05368753 0.00028493 0.00615059 0.0241404  0.07522282\n",
      " 0.         0.00022017 0.01658295 0.         0.00549898 0.01963998\n",
      " 0.01262551 0.00067978 0.         0.0032155  0.         0.\n",
      " 0.01889252 0.         0.         0.         0.01617809 0.\n",
      " 0.         0.         0.         0.01440031 0.         0.01232483\n",
      " 0.00932741 0.         0.         0.         0.         0.\n",
      " 0.04854589 0.         0.01445061 0.         0.         0.00445664\n",
      " 0.03085113 0.0272365  0.01732803 0.02029776]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Generate synthetic data with irrelevant features\n",
    "np.random.seed(0)\n",
    "X, y = make_regression(n_samples=100, n_features=100, n_informative=10, noise=0.1, random_state=0)\n",
    "\n",
    "# Introduce irrelevant features\n",
    "X[:, -20:] = np.random.random(size=(100, 20))\n",
    "\n",
    "# Split data into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "X_train, y_train = X[:split_index], y[:split_index]\n",
    "X_test, y_test = X[split_index:], y[split_index:]\n",
    "\n",
    "# Fit Kernel Ridge Regression\n",
    "kernel_reg = KernelRidge(kernel='rbf', gamma=0.01)\n",
    "kernel_reg.fit(X_train, y_train)\n",
    "pred = kernel_reg.predict(X_test) \n",
    "  \n",
    "# MSE Computation \n",
    "mse = MSE(y_test, pred)\n",
    "print(\"Test MSE Kernel:\", mse)\n",
    "\n",
    "# Fit XGBoost Regressor\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror',  max_depth=50, subsample=0.7,learning_rate = 0.001, alpha = 0,\n",
    "                  n_estimators = 10)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "pred = xgb_reg.predict(X_test) \n",
    "  \n",
    "# MSE Computation \n",
    "mse = MSE(y_test, pred)\n",
    "print(\"Test MSE XGB:\", mse)\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance_xgb = xgb_reg.feature_importances_\n",
    "\n",
    "print(\"Feature Importance (XGBoost):\", feature_importance_xgb)\n",
    "\n",
    "# Note: Kernel Ridge doesn't have built-in feature importance calculation\n",
    "\n",
    "# Here you can observe that XGBoost assigns lower importance to irrelevant features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
