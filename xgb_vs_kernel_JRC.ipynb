{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where xgboost works better\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import xgboost as xg \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal([0, 0], [[1, 0.9], [0.9, 1]], 10000)\n",
    "y= np.sin(X[:,0])/np.sqrt(np.abs(X[:,1])) + np.random.normal(0, 0.1, size=len(X))+0*np.random.standard_cauchy(size=10000)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                      test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.029007296155385224\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# Instantiation \n",
    "xgb_r = xg.XGBRegressor(objective ='reg:squarederror', max_depth=10, subsample=0.7,learning_rate = 0.5, alpha = 0,\n",
    "                  n_estimators = 10, seed = 123) \n",
    "  \n",
    "# Fitting the model \n",
    "xgb_r.fit(train_X, train_y) \n",
    "  \n",
    "# Predict the model \n",
    "pred = xgb_r.predict(test_X) \n",
    "  \n",
    "# MSE Computation \n",
    "mse = MSE(test_y, pred)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.973071572712237"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-mse/np.var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JYOTISHKA\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:255: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.2658606604328037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# Fit the kernel regression model\n",
    "alpha = 0  # Regularization parameter\n",
    "gamma = 0.01  # Kernel coefficient\n",
    "kernel_reg = KernelRidge(alpha=alpha, kernel='sigmoid', gamma=gamma)\n",
    "kernel_reg.fit(train_X, train_y)\n",
    "\n",
    "# Predict on the training and testing data\n",
    "#y_train_pred = kernel_reg.predict(X_train)\n",
    "y_test_pred = kernel_reg.predict(test_X)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "#mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse = MSE(test_y, y_test_pred)\n",
    "\n",
    "#print(\"Train MSE:\", mse_train)\n",
    "print(\"Test MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7531928027765424"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-mse/np.var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Kernel: 17163.161995852795\n",
      "Test MSE XGB: 19551.07077874765\n",
      "Feature Importance (XGBoost): [0.00080503 0.00320694 0.00950887 0.00410484 0.00199883 0.00385391\n",
      " 0.00357793 0.00951901 0.00048312 0.05670564 0.00148407 0.\n",
      " 0.         0.02004516 0.01032002 0.         0.00576737 0.01898836\n",
      " 0.02231819 0.00067819 0.00678883 0.01901409 0.         0.\n",
      " 0.0013739  0.10696716 0.02260618 0.00656806 0.00026808 0.01300839\n",
      " 0.         0.00385607 0.         0.027629   0.         0.\n",
      " 0.00432914 0.01457895 0.         0.00652961 0.00224158 0.07599161\n",
      " 0.         0.         0.0425243  0.01228812 0.02150544 0.\n",
      " 0.         0.01138969 0.         0.0062081  0.         0.01843056\n",
      " 0.         0.05568574 0.00706435 0.03223191 0.         0.01523276\n",
      " 0.         0.         0.         0.         0.01941507 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02062765 0.01729121 0.01565491 0.         0.         0.\n",
      " 0.         0.0298489  0.         0.         0.         0.01477738\n",
      " 0.08582543 0.01681902 0.02307868 0.         0.         0.0015661\n",
      " 0.01275819 0.         0.03466032 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Generate synthetic data with irrelevant features\n",
    "np.random.seed(0)\n",
    "X, y = make_regression(n_samples=100, n_features=100, n_informative=10, noise=0.1, random_state=0)\n",
    "\n",
    "# Introduce irrelevant features\n",
    "X[:, -20:] = np.random.random(size=(100, 20))\n",
    "\n",
    "# Split data into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "X_train, y_train = X[:split_index], y[:split_index]\n",
    "X_test, y_test = X[split_index:], y[split_index:]\n",
    "\n",
    "# Fit Kernel Ridge Regression\n",
    "kernel_reg = KernelRidge(kernel='rbf', gamma=0.01)\n",
    "kernel_reg.fit(X_train, y_train)\n",
    "pred = kernel_reg.predict(X_test) \n",
    "  \n",
    "# MSE Computation \n",
    "mse = MSE(y_test, pred)\n",
    "print(\"Test MSE Kernel:\", mse)\n",
    "\n",
    "# Fit XGBoost Regressor\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror',  max_depth=50, subsample=0.7,learning_rate = 0.001, alpha = 0,\n",
    "                  n_estimators = 10)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "pred = xgb_reg.predict(X_test) \n",
    "  \n",
    "# MSE Computation \n",
    "mse = MSE(y_test, pred)\n",
    "print(\"Test MSE XGB:\", mse)\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance_xgb = xgb_reg.feature_importances_\n",
    "\n",
    "print(\"Feature Importance (XGBoost):\", feature_importance_xgb)\n",
    "\n",
    "# Note: Kernel Ridge doesn't have built-in feature importance calculation\n",
    "\n",
    "# Here you can observe that XGBoost assigns lower importance to irrelevant features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
